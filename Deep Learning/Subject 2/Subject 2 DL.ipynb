{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf8ee932",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f663fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511af272",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fd19b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emg1</th>\n",
       "      <th>Emg2</th>\n",
       "      <th>Emg3</th>\n",
       "      <th>Emg4</th>\n",
       "      <th>Emg5</th>\n",
       "      <th>Emg6</th>\n",
       "      <th>Emg7</th>\n",
       "      <th>Emg8</th>\n",
       "      <th>Emg9</th>\n",
       "      <th>Emg10</th>\n",
       "      <th>repetition</th>\n",
       "      <th>rerepetition</th>\n",
       "      <th>stimulus</th>\n",
       "      <th>restimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178197</th>\n",
       "      <td>0.3467</td>\n",
       "      <td>0.0586</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.1392</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>0.4053</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334104</th>\n",
       "      <td>0.5566</td>\n",
       "      <td>0.5127</td>\n",
       "      <td>0.0610</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.3979</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1855</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289566</th>\n",
       "      <td>0.5127</td>\n",
       "      <td>0.0586</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0439</td>\n",
       "      <td>0.6812</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302541</th>\n",
       "      <td>0.1294</td>\n",
       "      <td>0.3296</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266208</th>\n",
       "      <td>1.1230</td>\n",
       "      <td>1.2793</td>\n",
       "      <td>0.6738</td>\n",
       "      <td>0.5127</td>\n",
       "      <td>0.2686</td>\n",
       "      <td>0.0513</td>\n",
       "      <td>0.1782</td>\n",
       "      <td>1.2915</td>\n",
       "      <td>0.2734</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Emg1     Emg2    Emg3    Emg4   Emg5     Emg6    Emg7    Emg8  \\\n",
       "178197  0.3467  0.0586  0.0317  0.1587  0.1392  0.0073  0.0952  0.4053   \n",
       "334104  0.5566  0.5127  0.0610  0.0024  0.0024  0.0024  0.0024  0.3979   \n",
       "289566  0.5127  0.0586  0.0049  0.0024  0.0024  0.0024  0.0439  0.6812   \n",
       "302541  0.1294  0.3296  0.0146  0.0024  0.0024  0.0049  0.0024  0.0342   \n",
       "266208  1.1230  1.2793  0.6738  0.5127  0.2686  0.0513  0.1782  1.2915   \n",
       "\n",
       "          Emg9   Emg10  repetition  rerepetition  stimulus  restimulus  \n",
       "178197  0.1587  0.3613           3             3        10          10  \n",
       "334104  0.1123  0.1855           5             5        10          10  \n",
       "289566  0.0317  0.4590           9             9         5           5  \n",
       "302541  0.0024  0.1123           0             0         0           0  \n",
       "266208  0.2734  0.8350           5             5         3           3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_excel('Dataset 1 Patient 2.xlsx')\n",
    "raw_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80369ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 472501 entries, 0 to 472500\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   Emg1          472501 non-null  float64\n",
      " 1   Emg2          472501 non-null  float64\n",
      " 2   Emg3          472501 non-null  float64\n",
      " 3   Emg4          472501 non-null  float64\n",
      " 4   Emg5          472501 non-null  float64\n",
      " 5   Emg6          472501 non-null  float64\n",
      " 6   Emg7          472501 non-null  float64\n",
      " 7   Emg8          472501 non-null  float64\n",
      " 8   Emg9          472501 non-null  float64\n",
      " 9   Emg10         472501 non-null  float64\n",
      " 10  repetition    472501 non-null  int64  \n",
      " 11  rerepetition  472501 non-null  int64  \n",
      " 12  stimulus      472501 non-null  int64  \n",
      " 13  restimulus    472501 non-null  int64  \n",
      "dtypes: float64(10), int64(4)\n",
      "memory usage: 50.5 MB\n"
     ]
    }
   ],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "109445f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emg1</th>\n",
       "      <th>Emg2</th>\n",
       "      <th>Emg3</th>\n",
       "      <th>Emg4</th>\n",
       "      <th>Emg5</th>\n",
       "      <th>Emg6</th>\n",
       "      <th>Emg7</th>\n",
       "      <th>Emg8</th>\n",
       "      <th>Emg9</th>\n",
       "      <th>Emg10</th>\n",
       "      <th>repetition</th>\n",
       "      <th>rerepetition</th>\n",
       "      <th>stimulus</th>\n",
       "      <th>restimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>472501.000000</td>\n",
       "      <td>472501.000000</td>\n",
       "      <td>472501.000000</td>\n",
       "      <td>472501.000000</td>\n",
       "      <td>472501.000000</td>\n",
       "      <td>472501.000000</td>\n",
       "      <td>472501.000000</td>\n",
       "      <td>472501.000000</td>\n",
       "      <td>472501.000000</td>\n",
       "      <td>472501.000000</td>\n",
       "      <td>472501.000000</td>\n",
       "      <td>472501.000000</td>\n",
       "      <td>472501.000000</td>\n",
       "      <td>472501.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.449598</td>\n",
       "      <td>0.369994</td>\n",
       "      <td>0.165067</td>\n",
       "      <td>0.110990</td>\n",
       "      <td>0.056133</td>\n",
       "      <td>0.014915</td>\n",
       "      <td>0.042496</td>\n",
       "      <td>0.302602</td>\n",
       "      <td>0.260029</td>\n",
       "      <td>0.219605</td>\n",
       "      <td>3.124415</td>\n",
       "      <td>2.194270</td>\n",
       "      <td>5.539897</td>\n",
       "      <td>3.961033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.549594</td>\n",
       "      <td>0.394968</td>\n",
       "      <td>0.285421</td>\n",
       "      <td>0.240335</td>\n",
       "      <td>0.265131</td>\n",
       "      <td>0.047967</td>\n",
       "      <td>0.110609</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>0.485920</td>\n",
       "      <td>0.424508</td>\n",
       "      <td>3.479845</td>\n",
       "      <td>3.242834</td>\n",
       "      <td>6.570169</td>\n",
       "      <td>6.186865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.100100</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.234400</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.590800</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>0.192900</td>\n",
       "      <td>0.092800</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.371100</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.665500</td>\n",
       "      <td>4.663100</td>\n",
       "      <td>3.886700</td>\n",
       "      <td>3.313000</td>\n",
       "      <td>4.663100</td>\n",
       "      <td>1.196300</td>\n",
       "      <td>1.621100</td>\n",
       "      <td>4.665500</td>\n",
       "      <td>4.663100</td>\n",
       "      <td>4.663100</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Emg1            Emg2           Emg3           Emg4  \\\n",
       "count  472501.000000  472501.000000  472501.000000  472501.000000   \n",
       "mean        0.449598       0.369994       0.165067       0.110990   \n",
       "std         0.549594       0.394968       0.285421       0.240335   \n",
       "min         0.002400       0.002400       0.002400       0.002400   \n",
       "25%         0.100100       0.114700       0.004900       0.002400   \n",
       "50%         0.234400       0.263700       0.043900       0.002400   \n",
       "75%         0.590800       0.502900       0.192900       0.092800   \n",
       "max         4.665500       4.663100       3.886700       3.313000   \n",
       "\n",
       "               Emg5            Emg6           Emg7           Emg8  \\\n",
       "count  472501.000000  472501.000000  472501.000000  472501.000000   \n",
       "mean        0.056133       0.014915       0.042496       0.302602   \n",
       "std         0.265131       0.047967       0.110609       0.505100   \n",
       "min         0.002400       0.000000       0.000000       0.002400   \n",
       "25%         0.002400       0.002400       0.002400       0.014600   \n",
       "50%         0.002400       0.002400       0.002400       0.095200   \n",
       "75%         0.009800       0.002400       0.024400       0.371100   \n",
       "max         4.663100       1.196300       1.621100       4.665500   \n",
       "\n",
       "                Emg9          Emg10     repetition   rerepetition  \\\n",
       "count  472501.000000  472501.000000  472501.000000  472501.000000   \n",
       "mean        0.260029       0.219605       3.124415       2.194270   \n",
       "std         0.485920       0.424508       3.479845       3.242834   \n",
       "min         0.000000       0.002400       0.000000       0.000000   \n",
       "25%         0.002400       0.002400       0.000000       0.000000   \n",
       "50%         0.002400       0.063500       2.000000       0.000000   \n",
       "75%         0.324700       0.244100       6.000000       4.000000   \n",
       "max         4.663100       4.663100      10.000000      10.000000   \n",
       "\n",
       "            stimulus     restimulus  \n",
       "count  472501.000000  472501.000000  \n",
       "mean        5.539897       3.961033  \n",
       "std         6.570169       6.186865  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         3.000000       0.000000  \n",
       "75%        10.000000       7.000000  \n",
       "max        23.000000      23.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b9a2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Dependent values and their counts :\n",
      "0     204101\n",
      "8      15513\n",
      "2      15511\n",
      "7      15508\n",
      "12     15504\n",
      "4      15495\n",
      "5      15490\n",
      "6      15478\n",
      "11     15473\n",
      "9      15471\n",
      "10     15448\n",
      "3      15445\n",
      "1      15427\n",
      "17     10345\n",
      "14     10345\n",
      "13     10341\n",
      "15     10313\n",
      "16     10282\n",
      "21      5194\n",
      "19      5185\n",
      "20      5178\n",
      "18      5176\n",
      "22      5151\n",
      "23      5127\n",
      "Name: stimulus, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Dependent values and their counts :\")\n",
    "print(raw_data[\"stimulus\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54420ec4",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a811198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = raw_data[ (raw_data['stimulus'] != raw_data['restimulus'])].index\n",
    "raw_data.drop(index_names, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "556cd92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = raw_data[ (raw_data['repetition'] != raw_data['rerepetition'])].index\n",
    "raw_data.drop(index_names, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c91744f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_data.iloc[:,0:10]\n",
    "y = raw_data.stimulus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f7160",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb77d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be64bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for categorical labels\n",
    "import keras\n",
    "from keras import utils as np_utils\n",
    "y = keras.utils.np_utils.to_categorical(y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7358f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3088a1",
   "metadata": {},
   "source": [
    "# Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bd820f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardscaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a118694",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pd.DataFrame(standardscaler.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6e2f38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.813756</td>\n",
       "      <td>-0.710291</td>\n",
       "      <td>-0.611593</td>\n",
       "      <td>-0.504274</td>\n",
       "      <td>-0.223238</td>\n",
       "      <td>-0.289385</td>\n",
       "      <td>-0.39416</td>\n",
       "      <td>-0.633016</td>\n",
       "      <td>-0.584607</td>\n",
       "      <td>-0.538362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.813756</td>\n",
       "      <td>-0.768316</td>\n",
       "      <td>-0.611593</td>\n",
       "      <td>-0.504274</td>\n",
       "      <td>-0.223238</td>\n",
       "      <td>-0.289385</td>\n",
       "      <td>-0.39416</td>\n",
       "      <td>-0.633016</td>\n",
       "      <td>-0.584607</td>\n",
       "      <td>-0.538362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.805569</td>\n",
       "      <td>-0.808981</td>\n",
       "      <td>-0.611593</td>\n",
       "      <td>-0.504274</td>\n",
       "      <td>-0.223238</td>\n",
       "      <td>-0.289385</td>\n",
       "      <td>-0.39416</td>\n",
       "      <td>-0.633016</td>\n",
       "      <td>-0.584607</td>\n",
       "      <td>-0.538362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.809662</td>\n",
       "      <td>-0.855353</td>\n",
       "      <td>-0.611593</td>\n",
       "      <td>-0.504274</td>\n",
       "      <td>-0.223238</td>\n",
       "      <td>-0.289385</td>\n",
       "      <td>-0.39416</td>\n",
       "      <td>-0.633016</td>\n",
       "      <td>-0.584607</td>\n",
       "      <td>-0.538362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.809662</td>\n",
       "      <td>-0.884366</td>\n",
       "      <td>-0.611593</td>\n",
       "      <td>-0.504274</td>\n",
       "      <td>-0.214670</td>\n",
       "      <td>-0.289385</td>\n",
       "      <td>-0.39416</td>\n",
       "      <td>-0.633016</td>\n",
       "      <td>-0.584607</td>\n",
       "      <td>-0.538362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386608</th>\n",
       "      <td>-0.614017</td>\n",
       "      <td>-0.007807</td>\n",
       "      <td>-0.524071</td>\n",
       "      <td>-0.504274</td>\n",
       "      <td>-0.223238</td>\n",
       "      <td>-0.289385</td>\n",
       "      <td>-0.39416</td>\n",
       "      <td>-0.619390</td>\n",
       "      <td>-0.584607</td>\n",
       "      <td>-0.538362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386609</th>\n",
       "      <td>-0.622204</td>\n",
       "      <td>-0.042527</td>\n",
       "      <td>-0.540014</td>\n",
       "      <td>-0.504274</td>\n",
       "      <td>-0.223238</td>\n",
       "      <td>-0.289385</td>\n",
       "      <td>-0.39416</td>\n",
       "      <td>-0.623994</td>\n",
       "      <td>-0.584607</td>\n",
       "      <td>-0.538362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386610</th>\n",
       "      <td>-0.609752</td>\n",
       "      <td>-0.007807</td>\n",
       "      <td>-0.547822</td>\n",
       "      <td>-0.494660</td>\n",
       "      <td>-0.223238</td>\n",
       "      <td>-0.289385</td>\n",
       "      <td>-0.39416</td>\n",
       "      <td>-0.619390</td>\n",
       "      <td>-0.584607</td>\n",
       "      <td>-0.538362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386611</th>\n",
       "      <td>-0.626468</td>\n",
       "      <td>-0.077484</td>\n",
       "      <td>-0.571899</td>\n",
       "      <td>-0.504274</td>\n",
       "      <td>-0.223238</td>\n",
       "      <td>-0.289385</td>\n",
       "      <td>-0.39416</td>\n",
       "      <td>-0.619390</td>\n",
       "      <td>-0.584607</td>\n",
       "      <td>-0.538362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386612</th>\n",
       "      <td>-0.605659</td>\n",
       "      <td>0.044511</td>\n",
       "      <td>-0.571899</td>\n",
       "      <td>-0.504274</td>\n",
       "      <td>-0.223238</td>\n",
       "      <td>-0.289385</td>\n",
       "      <td>-0.39416</td>\n",
       "      <td>-0.623994</td>\n",
       "      <td>-0.584607</td>\n",
       "      <td>-0.538362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>386613 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5        6  \\\n",
       "0      -0.813756 -0.710291 -0.611593 -0.504274 -0.223238 -0.289385 -0.39416   \n",
       "1      -0.813756 -0.768316 -0.611593 -0.504274 -0.223238 -0.289385 -0.39416   \n",
       "2      -0.805569 -0.808981 -0.611593 -0.504274 -0.223238 -0.289385 -0.39416   \n",
       "3      -0.809662 -0.855353 -0.611593 -0.504274 -0.223238 -0.289385 -0.39416   \n",
       "4      -0.809662 -0.884366 -0.611593 -0.504274 -0.214670 -0.289385 -0.39416   \n",
       "...          ...       ...       ...       ...       ...       ...      ...   \n",
       "386608 -0.614017 -0.007807 -0.524071 -0.504274 -0.223238 -0.289385 -0.39416   \n",
       "386609 -0.622204 -0.042527 -0.540014 -0.504274 -0.223238 -0.289385 -0.39416   \n",
       "386610 -0.609752 -0.007807 -0.547822 -0.494660 -0.223238 -0.289385 -0.39416   \n",
       "386611 -0.626468 -0.077484 -0.571899 -0.504274 -0.223238 -0.289385 -0.39416   \n",
       "386612 -0.605659  0.044511 -0.571899 -0.504274 -0.223238 -0.289385 -0.39416   \n",
       "\n",
       "               7         8         9  \n",
       "0      -0.633016 -0.584607 -0.538362  \n",
       "1      -0.633016 -0.584607 -0.538362  \n",
       "2      -0.633016 -0.584607 -0.538362  \n",
       "3      -0.633016 -0.584607 -0.538362  \n",
       "4      -0.633016 -0.584607 -0.538362  \n",
       "...          ...       ...       ...  \n",
       "386608 -0.619390 -0.584607 -0.538362  \n",
       "386609 -0.623994 -0.584607 -0.538362  \n",
       "386610 -0.619390 -0.584607 -0.538362  \n",
       "386611 -0.619390 -0.584607 -0.538362  \n",
       "386612 -0.623994 -0.584607 -0.538362  \n",
       "\n",
       "[386613 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65cfb75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(sc, y, test_size = 0.2, random_state = 39)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba47781",
   "metadata": {},
   "source": [
    "# Deep Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d426e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd0ef208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Convolution1D, Dropout\n",
    "from keras.initializers import random_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d61bc",
   "metadata": {},
   "source": [
    "# 1. Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd77bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 24\n",
    "input_dim = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbdce465",
   "metadata": {},
   "outputs": [],
   "source": [
    "visible = Input(shape=(input_dim,))\n",
    "hidden1 = Dense(3000, activation='relu')(visible)\n",
    "hidden2 = Dense(1500, activation='relu')(hidden1)\n",
    "hidden3 = Dropout(0.2)(hidden2)\n",
    "hidden4 = Dense(750, activation='relu')(hidden3)\n",
    "hidden5 = Dense(375, activation='relu')(hidden4)\n",
    "hidden6 = Dense(48, activation='relu')(hidden5)\n",
    "output = Dense(num_classes, activation='softmax')(hidden6)\n",
    "model = Model(inputs=visible, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dd33478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3000)              33000     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 750)               1125750   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 375)               281625    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 48)                18048     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 24)                1176      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,961,099\n",
      "Trainable params: 5,961,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3196ac97",
   "metadata": {},
   "source": [
    "# 2. Compile Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a2d558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1d2b36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, 'EMG_ANN', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3358b592",
   "metadata": {},
   "source": [
    "# 3. Fit Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0ed4ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "77/77 [==============================] - 8s 29ms/step - loss: 1.3018 - accuracy: 0.6507 - val_loss: 0.9304 - val_accuracy: 0.7310\n",
      "Epoch 2/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.8317 - accuracy: 0.7592 - val_loss: 0.7524 - val_accuracy: 0.7813\n",
      "Epoch 3/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.7170 - accuracy: 0.7904 - val_loss: 0.6825 - val_accuracy: 0.7995\n",
      "Epoch 4/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.6468 - accuracy: 0.8088 - val_loss: 0.6295 - val_accuracy: 0.8110\n",
      "Epoch 5/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.5958 - accuracy: 0.8232 - val_loss: 0.5716 - val_accuracy: 0.8298\n",
      "Epoch 6/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.5556 - accuracy: 0.8345 - val_loss: 0.5452 - val_accuracy: 0.8357\n",
      "Epoch 7/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.5230 - accuracy: 0.8434 - val_loss: 0.5006 - val_accuracy: 0.8487\n",
      "Epoch 8/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.4931 - accuracy: 0.8516 - val_loss: 0.4789 - val_accuracy: 0.8556\n",
      "Epoch 9/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.4690 - accuracy: 0.8581 - val_loss: 0.4536 - val_accuracy: 0.8630\n",
      "Epoch 10/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.4548 - accuracy: 0.8627 - val_loss: 0.4383 - val_accuracy: 0.8676\n",
      "Epoch 11/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.4299 - accuracy: 0.8695 - val_loss: 0.4287 - val_accuracy: 0.8694\n",
      "Epoch 12/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.4119 - accuracy: 0.8744 - val_loss: 0.4093 - val_accuracy: 0.8750\n",
      "Epoch 13/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.3983 - accuracy: 0.8787 - val_loss: 0.3996 - val_accuracy: 0.8769\n",
      "Epoch 14/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.3861 - accuracy: 0.8819 - val_loss: 0.3836 - val_accuracy: 0.8824\n",
      "Epoch 15/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.3732 - accuracy: 0.8857 - val_loss: 0.3837 - val_accuracy: 0.8849\n",
      "Epoch 16/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.3581 - accuracy: 0.8901 - val_loss: 0.3638 - val_accuracy: 0.8887\n",
      "Epoch 17/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.3492 - accuracy: 0.8924 - val_loss: 0.3526 - val_accuracy: 0.8918\n",
      "Epoch 18/300\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.3369 - accuracy: 0.8960 - val_loss: 0.3517 - val_accuracy: 0.8934\n",
      "Epoch 19/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.3278 - accuracy: 0.8985 - val_loss: 0.3354 - val_accuracy: 0.8958\n",
      "Epoch 20/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.3173 - accuracy: 0.9023 - val_loss: 0.3227 - val_accuracy: 0.9002\n",
      "Epoch 21/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.3088 - accuracy: 0.9047 - val_loss: 0.3257 - val_accuracy: 0.8994\n",
      "Epoch 22/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.3047 - accuracy: 0.9056 - val_loss: 0.3160 - val_accuracy: 0.9022\n",
      "Epoch 23/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.2918 - accuracy: 0.9098 - val_loss: 0.3048 - val_accuracy: 0.9062\n",
      "Epoch 24/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.2870 - accuracy: 0.9110 - val_loss: 0.3016 - val_accuracy: 0.9078\n",
      "Epoch 25/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2789 - accuracy: 0.9136 - val_loss: 0.3015 - val_accuracy: 0.9083\n",
      "Epoch 26/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2735 - accuracy: 0.9155 - val_loss: 0.2911 - val_accuracy: 0.9107\n",
      "Epoch 27/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2662 - accuracy: 0.9171 - val_loss: 0.2837 - val_accuracy: 0.9126\n",
      "Epoch 28/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2599 - accuracy: 0.9193 - val_loss: 0.2841 - val_accuracy: 0.9131\n",
      "Epoch 29/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2554 - accuracy: 0.9213 - val_loss: 0.2709 - val_accuracy: 0.9173\n",
      "Epoch 30/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2488 - accuracy: 0.9228 - val_loss: 0.2679 - val_accuracy: 0.9173\n",
      "Epoch 31/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2463 - accuracy: 0.9232 - val_loss: 0.2665 - val_accuracy: 0.9173\n",
      "Epoch 32/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2387 - accuracy: 0.9260 - val_loss: 0.2724 - val_accuracy: 0.9172\n",
      "Epoch 33/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2354 - accuracy: 0.9266 - val_loss: 0.2585 - val_accuracy: 0.9215\n",
      "Epoch 34/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2321 - accuracy: 0.9277 - val_loss: 0.2624 - val_accuracy: 0.9201\n",
      "Epoch 35/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2258 - accuracy: 0.9301 - val_loss: 0.2569 - val_accuracy: 0.9219\n",
      "Epoch 36/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2217 - accuracy: 0.9309 - val_loss: 0.2563 - val_accuracy: 0.9216\n",
      "Epoch 37/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2203 - accuracy: 0.9313 - val_loss: 0.2454 - val_accuracy: 0.9249\n",
      "Epoch 38/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.2136 - accuracy: 0.9336 - val_loss: 0.2380 - val_accuracy: 0.9267\n",
      "Epoch 39/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.2101 - accuracy: 0.9344 - val_loss: 0.2406 - val_accuracy: 0.9264\n",
      "Epoch 40/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.2067 - accuracy: 0.9354 - val_loss: 0.2410 - val_accuracy: 0.9263\n",
      "Epoch 41/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2031 - accuracy: 0.9367 - val_loss: 0.2356 - val_accuracy: 0.9272\n",
      "Epoch 42/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.2008 - accuracy: 0.9373 - val_loss: 0.2274 - val_accuracy: 0.9304\n",
      "Epoch 43/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1965 - accuracy: 0.9394 - val_loss: 0.2230 - val_accuracy: 0.9318\n",
      "Epoch 44/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1910 - accuracy: 0.9403 - val_loss: 0.2190 - val_accuracy: 0.9322\n",
      "Epoch 45/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1896 - accuracy: 0.9412 - val_loss: 0.2218 - val_accuracy: 0.9317\n",
      "Epoch 46/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1862 - accuracy: 0.9421 - val_loss: 0.2203 - val_accuracy: 0.9333\n",
      "Epoch 47/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1831 - accuracy: 0.9429 - val_loss: 0.2142 - val_accuracy: 0.9344\n",
      "Epoch 48/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.1843 - accuracy: 0.9429 - val_loss: 0.2230 - val_accuracy: 0.9337\n",
      "Epoch 49/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1822 - accuracy: 0.9431 - val_loss: 0.2132 - val_accuracy: 0.9349\n",
      "Epoch 50/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1782 - accuracy: 0.9442 - val_loss: 0.2135 - val_accuracy: 0.9346\n",
      "Epoch 51/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.1759 - accuracy: 0.9448 - val_loss: 0.2127 - val_accuracy: 0.9361\n",
      "Epoch 52/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1727 - accuracy: 0.9463 - val_loss: 0.2123 - val_accuracy: 0.9362\n",
      "Epoch 53/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.1707 - accuracy: 0.9465 - val_loss: 0.2106 - val_accuracy: 0.9356\n",
      "Epoch 54/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.1706 - accuracy: 0.9467 - val_loss: 0.2068 - val_accuracy: 0.9365\n",
      "Epoch 55/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1666 - accuracy: 0.9482 - val_loss: 0.2082 - val_accuracy: 0.9370\n",
      "Epoch 56/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1642 - accuracy: 0.9490 - val_loss: 0.2016 - val_accuracy: 0.9385\n",
      "Epoch 57/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1631 - accuracy: 0.9486 - val_loss: 0.2068 - val_accuracy: 0.9378\n",
      "Epoch 58/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1602 - accuracy: 0.9499 - val_loss: 0.2053 - val_accuracy: 0.9384\n",
      "Epoch 59/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1564 - accuracy: 0.9515 - val_loss: 0.1920 - val_accuracy: 0.9414\n",
      "Epoch 60/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.1532 - accuracy: 0.9525 - val_loss: 0.1944 - val_accuracy: 0.9414\n",
      "Epoch 61/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1522 - accuracy: 0.9524 - val_loss: 0.1974 - val_accuracy: 0.9403\n",
      "Epoch 62/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1508 - accuracy: 0.9529 - val_loss: 0.2017 - val_accuracy: 0.9382\n",
      "Epoch 63/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1524 - accuracy: 0.9522 - val_loss: 0.1918 - val_accuracy: 0.9429\n",
      "Epoch 64/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1515 - accuracy: 0.9529 - val_loss: 0.1904 - val_accuracy: 0.9427\n",
      "Epoch 65/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1486 - accuracy: 0.9535 - val_loss: 0.1916 - val_accuracy: 0.9423\n",
      "Epoch 66/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.1442 - accuracy: 0.9551 - val_loss: 0.1912 - val_accuracy: 0.9419\n",
      "Epoch 67/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.1434 - accuracy: 0.9550 - val_loss: 0.1899 - val_accuracy: 0.9421\n",
      "Epoch 68/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.1439 - accuracy: 0.9547 - val_loss: 0.1907 - val_accuracy: 0.9427\n",
      "Epoch 69/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1435 - accuracy: 0.9548 - val_loss: 0.1885 - val_accuracy: 0.9430\n",
      "Epoch 70/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1396 - accuracy: 0.9561 - val_loss: 0.1861 - val_accuracy: 0.9437\n",
      "Epoch 71/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1408 - accuracy: 0.9562 - val_loss: 0.1890 - val_accuracy: 0.9431\n",
      "Epoch 72/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1382 - accuracy: 0.9568 - val_loss: 0.1819 - val_accuracy: 0.9453\n",
      "Epoch 73/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1345 - accuracy: 0.9579 - val_loss: 0.1858 - val_accuracy: 0.9446\n",
      "Epoch 74/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1348 - accuracy: 0.9577 - val_loss: 0.1865 - val_accuracy: 0.9450\n",
      "Epoch 75/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1331 - accuracy: 0.9583 - val_loss: 0.1792 - val_accuracy: 0.9466\n",
      "Epoch 76/300\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.1323 - accuracy: 0.9582 - val_loss: 0.1772 - val_accuracy: 0.9476\n",
      "Epoch 77/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1292 - accuracy: 0.9596 - val_loss: 0.1826 - val_accuracy: 0.9455\n",
      "Epoch 78/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1292 - accuracy: 0.9595 - val_loss: 0.1756 - val_accuracy: 0.9477\n",
      "Epoch 79/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1263 - accuracy: 0.9602 - val_loss: 0.1768 - val_accuracy: 0.9471\n",
      "Epoch 80/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1267 - accuracy: 0.9601 - val_loss: 0.1725 - val_accuracy: 0.9496\n",
      "Epoch 81/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1254 - accuracy: 0.9606 - val_loss: 0.1715 - val_accuracy: 0.9495\n",
      "Epoch 82/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1240 - accuracy: 0.9613 - val_loss: 0.1793 - val_accuracy: 0.9471\n",
      "Epoch 83/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1253 - accuracy: 0.9610 - val_loss: 0.1735 - val_accuracy: 0.9480\n",
      "Epoch 84/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1217 - accuracy: 0.9619 - val_loss: 0.1673 - val_accuracy: 0.9503\n",
      "Epoch 85/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1211 - accuracy: 0.9619 - val_loss: 0.1737 - val_accuracy: 0.9482\n",
      "Epoch 86/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1214 - accuracy: 0.9619 - val_loss: 0.1687 - val_accuracy: 0.9501\n",
      "Epoch 87/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1192 - accuracy: 0.9626 - val_loss: 0.1687 - val_accuracy: 0.9493\n",
      "Epoch 88/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1206 - accuracy: 0.9625 - val_loss: 0.1784 - val_accuracy: 0.9476\n",
      "Epoch 89/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1190 - accuracy: 0.9623 - val_loss: 0.1712 - val_accuracy: 0.9496\n",
      "Epoch 90/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1160 - accuracy: 0.9638 - val_loss: 0.1727 - val_accuracy: 0.9499\n",
      "Epoch 91/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1167 - accuracy: 0.9634 - val_loss: 0.1684 - val_accuracy: 0.9514\n",
      "Epoch 92/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1213 - accuracy: 0.9621 - val_loss: 0.1672 - val_accuracy: 0.9513\n",
      "Epoch 93/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1131 - accuracy: 0.9645 - val_loss: 0.1670 - val_accuracy: 0.9511\n",
      "Epoch 94/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1137 - accuracy: 0.9641 - val_loss: 0.1671 - val_accuracy: 0.9509\n",
      "Epoch 95/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1143 - accuracy: 0.9645 - val_loss: 0.1624 - val_accuracy: 0.9518\n",
      "Epoch 96/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1120 - accuracy: 0.9648 - val_loss: 0.1670 - val_accuracy: 0.9522\n",
      "Epoch 97/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1096 - accuracy: 0.9657 - val_loss: 0.1673 - val_accuracy: 0.9516\n",
      "Epoch 98/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1119 - accuracy: 0.9647 - val_loss: 0.1670 - val_accuracy: 0.9525\n",
      "Epoch 99/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1114 - accuracy: 0.9650 - val_loss: 0.1707 - val_accuracy: 0.9504\n",
      "Epoch 100/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1106 - accuracy: 0.9655 - val_loss: 0.1613 - val_accuracy: 0.9532\n",
      "Epoch 101/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1078 - accuracy: 0.9659 - val_loss: 0.1597 - val_accuracy: 0.9528\n",
      "Epoch 102/300\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.1055 - accuracy: 0.9669 - val_loss: 0.1723 - val_accuracy: 0.9520\n",
      "Epoch 103/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1054 - accuracy: 0.9670 - val_loss: 0.1639 - val_accuracy: 0.9527\n",
      "Epoch 104/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1057 - accuracy: 0.9669 - val_loss: 0.1628 - val_accuracy: 0.9531\n",
      "Epoch 105/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1058 - accuracy: 0.9669 - val_loss: 0.1648 - val_accuracy: 0.9533\n",
      "Epoch 106/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1030 - accuracy: 0.9676 - val_loss: 0.1597 - val_accuracy: 0.9529\n",
      "Epoch 107/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1016 - accuracy: 0.9678 - val_loss: 0.1714 - val_accuracy: 0.9526\n",
      "Epoch 108/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1045 - accuracy: 0.9673 - val_loss: 0.1563 - val_accuracy: 0.9536\n",
      "Epoch 109/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1014 - accuracy: 0.9684 - val_loss: 0.1575 - val_accuracy: 0.9543\n",
      "Epoch 110/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0988 - accuracy: 0.9689 - val_loss: 0.1590 - val_accuracy: 0.9542\n",
      "Epoch 111/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0984 - accuracy: 0.9691 - val_loss: 0.1679 - val_accuracy: 0.9531\n",
      "Epoch 112/300\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.1005 - accuracy: 0.9685 - val_loss: 0.1586 - val_accuracy: 0.9549\n",
      "Epoch 113/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1010 - accuracy: 0.9682 - val_loss: 0.1617 - val_accuracy: 0.9541\n",
      "Epoch 114/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1014 - accuracy: 0.9685 - val_loss: 0.1576 - val_accuracy: 0.9544\n",
      "Epoch 115/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1013 - accuracy: 0.9681 - val_loss: 0.1618 - val_accuracy: 0.9533\n",
      "Epoch 116/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0992 - accuracy: 0.9688 - val_loss: 0.1636 - val_accuracy: 0.9536\n",
      "Epoch 117/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0973 - accuracy: 0.9696 - val_loss: 0.1555 - val_accuracy: 0.9550\n",
      "Epoch 118/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0954 - accuracy: 0.9700 - val_loss: 0.1735 - val_accuracy: 0.9503\n",
      "Epoch 119/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0975 - accuracy: 0.9692 - val_loss: 0.1606 - val_accuracy: 0.9538\n",
      "Epoch 120/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1001 - accuracy: 0.9687 - val_loss: 0.1590 - val_accuracy: 0.9548\n",
      "Epoch 121/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0954 - accuracy: 0.9699 - val_loss: 0.1605 - val_accuracy: 0.9547\n",
      "Epoch 122/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0934 - accuracy: 0.9707 - val_loss: 0.1575 - val_accuracy: 0.9565\n",
      "Epoch 123/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0962 - accuracy: 0.9702 - val_loss: 0.1575 - val_accuracy: 0.9552\n",
      "Epoch 124/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0958 - accuracy: 0.9700 - val_loss: 0.1561 - val_accuracy: 0.9567\n",
      "Epoch 125/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0928 - accuracy: 0.9709 - val_loss: 0.1644 - val_accuracy: 0.9542\n",
      "Epoch 126/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0945 - accuracy: 0.9703 - val_loss: 0.1579 - val_accuracy: 0.9560\n",
      "Epoch 127/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0909 - accuracy: 0.9714 - val_loss: 0.1604 - val_accuracy: 0.9550\n",
      "Epoch 128/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0902 - accuracy: 0.9720 - val_loss: 0.1581 - val_accuracy: 0.9552\n",
      "Epoch 129/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0924 - accuracy: 0.9709 - val_loss: 0.1552 - val_accuracy: 0.9563\n",
      "Epoch 130/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0914 - accuracy: 0.9714 - val_loss: 0.1577 - val_accuracy: 0.9569\n",
      "Epoch 131/300\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.0907 - accuracy: 0.9716 - val_loss: 0.1532 - val_accuracy: 0.9569\n",
      "Epoch 132/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0873 - accuracy: 0.9726 - val_loss: 0.1585 - val_accuracy: 0.9563\n",
      "Epoch 133/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0876 - accuracy: 0.9725 - val_loss: 0.1522 - val_accuracy: 0.9570\n",
      "Epoch 134/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0884 - accuracy: 0.9723 - val_loss: 0.1594 - val_accuracy: 0.9578\n",
      "Epoch 135/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0886 - accuracy: 0.9722 - val_loss: 0.1551 - val_accuracy: 0.9564\n",
      "Epoch 136/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0873 - accuracy: 0.9726 - val_loss: 0.1608 - val_accuracy: 0.9567\n",
      "Epoch 137/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0867 - accuracy: 0.9724 - val_loss: 0.1531 - val_accuracy: 0.9577\n",
      "Epoch 138/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0845 - accuracy: 0.9733 - val_loss: 0.1526 - val_accuracy: 0.9564\n",
      "Epoch 139/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0891 - accuracy: 0.9721 - val_loss: 0.1564 - val_accuracy: 0.9559\n",
      "Epoch 140/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0856 - accuracy: 0.9732 - val_loss: 0.1584 - val_accuracy: 0.9566\n",
      "Epoch 141/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0834 - accuracy: 0.9737 - val_loss: 0.1591 - val_accuracy: 0.9561\n",
      "Epoch 142/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0827 - accuracy: 0.9739 - val_loss: 0.1577 - val_accuracy: 0.9571\n",
      "Epoch 143/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0849 - accuracy: 0.9735 - val_loss: 0.1565 - val_accuracy: 0.9564\n",
      "Epoch 144/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0845 - accuracy: 0.9735 - val_loss: 0.1572 - val_accuracy: 0.9573\n",
      "Epoch 145/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0820 - accuracy: 0.9742 - val_loss: 0.1523 - val_accuracy: 0.9588\n",
      "Epoch 146/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0808 - accuracy: 0.9744 - val_loss: 0.1557 - val_accuracy: 0.9575\n",
      "Epoch 147/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0846 - accuracy: 0.9734 - val_loss: 0.1544 - val_accuracy: 0.9572\n",
      "Epoch 148/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0839 - accuracy: 0.9738 - val_loss: 0.1546 - val_accuracy: 0.9583\n",
      "Epoch 149/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0830 - accuracy: 0.9741 - val_loss: 0.1545 - val_accuracy: 0.9580\n",
      "Epoch 150/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0827 - accuracy: 0.9741 - val_loss: 0.1542 - val_accuracy: 0.9585\n",
      "Epoch 151/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0814 - accuracy: 0.9747 - val_loss: 0.1521 - val_accuracy: 0.9587\n",
      "Epoch 152/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0810 - accuracy: 0.9744 - val_loss: 0.1506 - val_accuracy: 0.9591\n",
      "Epoch 153/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0793 - accuracy: 0.9748 - val_loss: 0.1507 - val_accuracy: 0.9596\n",
      "Epoch 154/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0804 - accuracy: 0.9747 - val_loss: 0.1532 - val_accuracy: 0.9582\n",
      "Epoch 155/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0793 - accuracy: 0.9751 - val_loss: 0.1485 - val_accuracy: 0.9596\n",
      "Epoch 156/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0783 - accuracy: 0.9755 - val_loss: 0.1488 - val_accuracy: 0.9592\n",
      "Epoch 157/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0807 - accuracy: 0.9748 - val_loss: 0.1582 - val_accuracy: 0.9561\n",
      "Epoch 158/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0832 - accuracy: 0.9738 - val_loss: 0.1558 - val_accuracy: 0.9586\n",
      "Epoch 159/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0790 - accuracy: 0.9753 - val_loss: 0.1493 - val_accuracy: 0.9598\n",
      "Epoch 160/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0786 - accuracy: 0.9754 - val_loss: 0.1506 - val_accuracy: 0.9589\n",
      "Epoch 161/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0786 - accuracy: 0.9753 - val_loss: 0.1528 - val_accuracy: 0.9586\n",
      "Epoch 162/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0775 - accuracy: 0.9756 - val_loss: 0.1492 - val_accuracy: 0.9596\n",
      "Epoch 163/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0760 - accuracy: 0.9758 - val_loss: 0.1584 - val_accuracy: 0.9584\n",
      "Epoch 164/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0778 - accuracy: 0.9759 - val_loss: 0.1518 - val_accuracy: 0.9591\n",
      "Epoch 165/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0749 - accuracy: 0.9766 - val_loss: 0.1529 - val_accuracy: 0.9591\n",
      "Epoch 166/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0774 - accuracy: 0.9758 - val_loss: 0.1529 - val_accuracy: 0.9577\n",
      "Epoch 167/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0752 - accuracy: 0.9764 - val_loss: 0.1541 - val_accuracy: 0.9582\n",
      "Epoch 168/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0754 - accuracy: 0.9762 - val_loss: 0.1480 - val_accuracy: 0.9603\n",
      "Epoch 169/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0756 - accuracy: 0.9760 - val_loss: 0.1527 - val_accuracy: 0.9598\n",
      "Epoch 170/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0727 - accuracy: 0.9772 - val_loss: 0.1522 - val_accuracy: 0.9603\n",
      "Epoch 171/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0738 - accuracy: 0.9768 - val_loss: 0.1519 - val_accuracy: 0.9604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0746 - accuracy: 0.9763 - val_loss: 0.1499 - val_accuracy: 0.9600\n",
      "Epoch 173/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0776 - accuracy: 0.9757 - val_loss: 0.1531 - val_accuracy: 0.9586\n",
      "Epoch 174/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0733 - accuracy: 0.9771 - val_loss: 0.1472 - val_accuracy: 0.9599\n",
      "Epoch 175/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0695 - accuracy: 0.9781 - val_loss: 0.1541 - val_accuracy: 0.9598\n",
      "Epoch 176/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0735 - accuracy: 0.9769 - val_loss: 0.1480 - val_accuracy: 0.9607\n",
      "Epoch 177/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0698 - accuracy: 0.9781 - val_loss: 0.1565 - val_accuracy: 0.9597\n",
      "Epoch 178/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0704 - accuracy: 0.9779 - val_loss: 0.1421 - val_accuracy: 0.9618\n",
      "Epoch 179/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0722 - accuracy: 0.9773 - val_loss: 0.1546 - val_accuracy: 0.9591\n",
      "Epoch 180/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0758 - accuracy: 0.9763 - val_loss: 0.1537 - val_accuracy: 0.9593\n",
      "Epoch 181/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0712 - accuracy: 0.9775 - val_loss: 0.1471 - val_accuracy: 0.9607\n",
      "Epoch 182/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0705 - accuracy: 0.9776 - val_loss: 0.1496 - val_accuracy: 0.9605\n",
      "Epoch 183/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0720 - accuracy: 0.9771 - val_loss: 0.1583 - val_accuracy: 0.9587\n",
      "Epoch 184/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0739 - accuracy: 0.9771 - val_loss: 0.1477 - val_accuracy: 0.9611\n",
      "Epoch 185/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0725 - accuracy: 0.9774 - val_loss: 0.1505 - val_accuracy: 0.9606\n",
      "Epoch 186/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0715 - accuracy: 0.9776 - val_loss: 0.1483 - val_accuracy: 0.9608\n",
      "Epoch 187/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0700 - accuracy: 0.9781 - val_loss: 0.1540 - val_accuracy: 0.9595\n",
      "Epoch 188/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0697 - accuracy: 0.9780 - val_loss: 0.1502 - val_accuracy: 0.9614\n",
      "Epoch 189/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0693 - accuracy: 0.9785 - val_loss: 0.1547 - val_accuracy: 0.9595\n",
      "Epoch 190/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0696 - accuracy: 0.9781 - val_loss: 0.1552 - val_accuracy: 0.9597\n",
      "Epoch 191/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0673 - accuracy: 0.9788 - val_loss: 0.1591 - val_accuracy: 0.9595\n",
      "Epoch 192/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0704 - accuracy: 0.9779 - val_loss: 0.1479 - val_accuracy: 0.9613\n",
      "Epoch 193/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0676 - accuracy: 0.9787 - val_loss: 0.1531 - val_accuracy: 0.9605\n",
      "Epoch 194/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0674 - accuracy: 0.9791 - val_loss: 0.1526 - val_accuracy: 0.9613\n",
      "Epoch 195/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0674 - accuracy: 0.9787 - val_loss: 0.1537 - val_accuracy: 0.9600\n",
      "Epoch 196/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0663 - accuracy: 0.9794 - val_loss: 0.1476 - val_accuracy: 0.9622\n",
      "Epoch 197/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0676 - accuracy: 0.9788 - val_loss: 0.1515 - val_accuracy: 0.9613\n",
      "Epoch 198/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0682 - accuracy: 0.9788 - val_loss: 0.1518 - val_accuracy: 0.9611\n",
      "Epoch 199/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0703 - accuracy: 0.9777 - val_loss: 0.1584 - val_accuracy: 0.9600\n",
      "Epoch 200/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0699 - accuracy: 0.9784 - val_loss: 0.1579 - val_accuracy: 0.9598\n",
      "Epoch 201/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0644 - accuracy: 0.9800 - val_loss: 0.1536 - val_accuracy: 0.9618\n",
      "Epoch 202/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0686 - accuracy: 0.9786 - val_loss: 0.1527 - val_accuracy: 0.9611\n",
      "Epoch 203/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0646 - accuracy: 0.9799 - val_loss: 0.1476 - val_accuracy: 0.9620\n",
      "Epoch 204/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0628 - accuracy: 0.9801 - val_loss: 0.1501 - val_accuracy: 0.9613\n",
      "Epoch 205/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0645 - accuracy: 0.9800 - val_loss: 0.1561 - val_accuracy: 0.9610\n",
      "Epoch 206/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0657 - accuracy: 0.9794 - val_loss: 0.1509 - val_accuracy: 0.9614\n",
      "Epoch 207/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0654 - accuracy: 0.9797 - val_loss: 0.1535 - val_accuracy: 0.9609\n",
      "Epoch 208/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0674 - accuracy: 0.9791 - val_loss: 0.1536 - val_accuracy: 0.9615\n",
      "Epoch 209/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0647 - accuracy: 0.9797 - val_loss: 0.1578 - val_accuracy: 0.9599\n",
      "Epoch 210/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0655 - accuracy: 0.9796 - val_loss: 0.1531 - val_accuracy: 0.9623\n",
      "Epoch 211/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0623 - accuracy: 0.9807 - val_loss: 0.1517 - val_accuracy: 0.9622\n",
      "Epoch 212/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0638 - accuracy: 0.9801 - val_loss: 0.1549 - val_accuracy: 0.9606\n",
      "Epoch 213/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0628 - accuracy: 0.9804 - val_loss: 0.1558 - val_accuracy: 0.9605\n",
      "Epoch 214/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0658 - accuracy: 0.9795 - val_loss: 0.1532 - val_accuracy: 0.9608\n",
      "Epoch 215/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0632 - accuracy: 0.9803 - val_loss: 0.1472 - val_accuracy: 0.9623\n",
      "Epoch 216/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0612 - accuracy: 0.9809 - val_loss: 0.1559 - val_accuracy: 0.9612\n",
      "Epoch 217/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0639 - accuracy: 0.9800 - val_loss: 0.1536 - val_accuracy: 0.9610\n",
      "Epoch 218/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0645 - accuracy: 0.9795 - val_loss: 0.1531 - val_accuracy: 0.9606\n",
      "Epoch 219/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0628 - accuracy: 0.9804 - val_loss: 0.1524 - val_accuracy: 0.9623\n",
      "Epoch 220/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0616 - accuracy: 0.9804 - val_loss: 0.1529 - val_accuracy: 0.9619\n",
      "Epoch 221/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0620 - accuracy: 0.9806 - val_loss: 0.1565 - val_accuracy: 0.9614\n",
      "Epoch 222/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0644 - accuracy: 0.9795 - val_loss: 0.1507 - val_accuracy: 0.9616\n",
      "Epoch 223/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0615 - accuracy: 0.9809 - val_loss: 0.1508 - val_accuracy: 0.9617\n",
      "Epoch 224/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0598 - accuracy: 0.9814 - val_loss: 0.1482 - val_accuracy: 0.9616\n",
      "Epoch 225/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0597 - accuracy: 0.9815 - val_loss: 0.1474 - val_accuracy: 0.9627\n",
      "Epoch 226/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0619 - accuracy: 0.9808 - val_loss: 0.1512 - val_accuracy: 0.9629\n",
      "Epoch 227/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0605 - accuracy: 0.9813 - val_loss: 0.1492 - val_accuracy: 0.9628\n",
      "Epoch 228/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0585 - accuracy: 0.9816 - val_loss: 0.1455 - val_accuracy: 0.9627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0616 - accuracy: 0.9810 - val_loss: 0.1562 - val_accuracy: 0.9621\n",
      "Epoch 230/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0625 - accuracy: 0.9804 - val_loss: 0.1489 - val_accuracy: 0.9622\n",
      "Epoch 231/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0587 - accuracy: 0.9816 - val_loss: 0.1490 - val_accuracy: 0.9624\n",
      "Epoch 232/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0581 - accuracy: 0.9820 - val_loss: 0.1544 - val_accuracy: 0.9633\n",
      "Epoch 233/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0686 - accuracy: 0.9788 - val_loss: 0.1581 - val_accuracy: 0.9605\n",
      "Epoch 234/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0635 - accuracy: 0.9802 - val_loss: 0.1464 - val_accuracy: 0.9627\n",
      "Epoch 235/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0599 - accuracy: 0.9811 - val_loss: 0.1515 - val_accuracy: 0.9630\n",
      "Epoch 236/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0571 - accuracy: 0.9822 - val_loss: 0.1549 - val_accuracy: 0.9619\n",
      "Epoch 237/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0613 - accuracy: 0.9808 - val_loss: 0.1553 - val_accuracy: 0.9612\n",
      "Epoch 238/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0585 - accuracy: 0.9819 - val_loss: 0.1533 - val_accuracy: 0.9626\n",
      "Epoch 239/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0572 - accuracy: 0.9819 - val_loss: 0.1621 - val_accuracy: 0.9614\n",
      "Epoch 240/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0612 - accuracy: 0.9810 - val_loss: 0.1485 - val_accuracy: 0.9630\n",
      "Epoch 241/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0578 - accuracy: 0.9819 - val_loss: 0.1449 - val_accuracy: 0.9637\n",
      "Epoch 242/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0571 - accuracy: 0.9822 - val_loss: 0.1510 - val_accuracy: 0.9625\n",
      "Epoch 243/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0571 - accuracy: 0.9823 - val_loss: 0.1531 - val_accuracy: 0.9629\n",
      "Epoch 244/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0596 - accuracy: 0.9816 - val_loss: 0.1465 - val_accuracy: 0.9633\n",
      "Epoch 245/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0569 - accuracy: 0.9822 - val_loss: 0.1546 - val_accuracy: 0.9629\n",
      "Epoch 246/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0595 - accuracy: 0.9816 - val_loss: 0.1536 - val_accuracy: 0.9620\n",
      "Epoch 247/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0592 - accuracy: 0.9815 - val_loss: 0.1616 - val_accuracy: 0.9614\n",
      "Epoch 248/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0595 - accuracy: 0.9816 - val_loss: 0.1550 - val_accuracy: 0.9630\n",
      "Epoch 249/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0576 - accuracy: 0.9821 - val_loss: 0.1553 - val_accuracy: 0.9618\n",
      "Epoch 250/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0572 - accuracy: 0.9822 - val_loss: 0.1564 - val_accuracy: 0.9629\n",
      "Epoch 251/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0597 - accuracy: 0.9815 - val_loss: 0.1624 - val_accuracy: 0.9601\n",
      "Epoch 252/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0565 - accuracy: 0.9823 - val_loss: 0.1505 - val_accuracy: 0.9631\n",
      "Epoch 253/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.1549 - val_accuracy: 0.9628\n",
      "Epoch 254/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0546 - accuracy: 0.9826 - val_loss: 0.1572 - val_accuracy: 0.9621\n",
      "Epoch 255/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0571 - accuracy: 0.9823 - val_loss: 0.1521 - val_accuracy: 0.9627\n",
      "Epoch 256/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0549 - accuracy: 0.9829 - val_loss: 0.1519 - val_accuracy: 0.9624\n",
      "Epoch 257/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0519 - accuracy: 0.9836 - val_loss: 0.1614 - val_accuracy: 0.9615\n",
      "Epoch 258/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0536 - accuracy: 0.9830 - val_loss: 0.1499 - val_accuracy: 0.9642\n",
      "Epoch 259/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0551 - accuracy: 0.9829 - val_loss: 0.1622 - val_accuracy: 0.9610\n",
      "Epoch 260/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0585 - accuracy: 0.9818 - val_loss: 0.1521 - val_accuracy: 0.9638\n",
      "Epoch 261/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0590 - accuracy: 0.9816 - val_loss: 0.1536 - val_accuracy: 0.9633\n",
      "Epoch 262/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0568 - accuracy: 0.9822 - val_loss: 0.1523 - val_accuracy: 0.9633\n",
      "Epoch 263/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0547 - accuracy: 0.9829 - val_loss: 0.1601 - val_accuracy: 0.9623\n",
      "Epoch 264/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0568 - accuracy: 0.9824 - val_loss: 0.1535 - val_accuracy: 0.9624\n",
      "Epoch 265/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0543 - accuracy: 0.9830 - val_loss: 0.1546 - val_accuracy: 0.9635\n",
      "Epoch 266/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0540 - accuracy: 0.9834 - val_loss: 0.1518 - val_accuracy: 0.9639\n",
      "Epoch 267/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0561 - accuracy: 0.9825 - val_loss: 0.1486 - val_accuracy: 0.9631\n",
      "Epoch 268/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0549 - accuracy: 0.9831 - val_loss: 0.1583 - val_accuracy: 0.9611\n",
      "Epoch 269/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0537 - accuracy: 0.9832 - val_loss: 0.1553 - val_accuracy: 0.9635\n",
      "Epoch 270/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0538 - accuracy: 0.9834 - val_loss: 0.1550 - val_accuracy: 0.9627\n",
      "Epoch 271/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0524 - accuracy: 0.9837 - val_loss: 0.1526 - val_accuracy: 0.9644\n",
      "Epoch 272/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0568 - accuracy: 0.9822 - val_loss: 0.1582 - val_accuracy: 0.9626\n",
      "Epoch 273/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0533 - accuracy: 0.9830 - val_loss: 0.1492 - val_accuracy: 0.9641\n",
      "Epoch 274/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0542 - accuracy: 0.9834 - val_loss: 0.1601 - val_accuracy: 0.9619\n",
      "Epoch 275/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0544 - accuracy: 0.9832 - val_loss: 0.1582 - val_accuracy: 0.9615\n",
      "Epoch 276/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0564 - accuracy: 0.9825 - val_loss: 0.1494 - val_accuracy: 0.9630\n",
      "Epoch 277/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0515 - accuracy: 0.9840 - val_loss: 0.1512 - val_accuracy: 0.9646\n",
      "Epoch 278/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0535 - accuracy: 0.9836 - val_loss: 0.1532 - val_accuracy: 0.9632\n",
      "Epoch 279/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0539 - accuracy: 0.9834 - val_loss: 0.1539 - val_accuracy: 0.9637\n",
      "Epoch 280/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0534 - accuracy: 0.9836 - val_loss: 0.1532 - val_accuracy: 0.9635\n",
      "Epoch 281/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0534 - accuracy: 0.9834 - val_loss: 0.1483 - val_accuracy: 0.9638\n",
      "Epoch 282/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0510 - accuracy: 0.9840 - val_loss: 0.1547 - val_accuracy: 0.9627\n",
      "Epoch 283/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0504 - accuracy: 0.9843 - val_loss: 0.1511 - val_accuracy: 0.9642\n",
      "Epoch 284/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0503 - accuracy: 0.9843 - val_loss: 0.1480 - val_accuracy: 0.9643\n",
      "Epoch 285/300\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.0511 - accuracy: 0.9842 - val_loss: 0.1533 - val_accuracy: 0.9647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0506 - accuracy: 0.9843 - val_loss: 0.1591 - val_accuracy: 0.9641\n",
      "Epoch 287/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0531 - accuracy: 0.9833 - val_loss: 0.1580 - val_accuracy: 0.9634\n",
      "Epoch 288/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0517 - accuracy: 0.9838 - val_loss: 0.1519 - val_accuracy: 0.9648\n",
      "Epoch 289/300\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.0520 - accuracy: 0.9838 - val_loss: 0.1629 - val_accuracy: 0.9631\n",
      "Epoch 290/300\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0529 - accuracy: 0.9835 - val_loss: 0.1522 - val_accuracy: 0.9644\n",
      "Epoch 291/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0516 - accuracy: 0.9839 - val_loss: 0.1548 - val_accuracy: 0.9641\n",
      "Epoch 292/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0498 - accuracy: 0.9846 - val_loss: 0.1517 - val_accuracy: 0.9641\n",
      "Epoch 293/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0496 - accuracy: 0.9845 - val_loss: 0.1551 - val_accuracy: 0.9646\n",
      "Epoch 294/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0496 - accuracy: 0.9846 - val_loss: 0.1496 - val_accuracy: 0.9648\n",
      "Epoch 295/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0516 - accuracy: 0.9838 - val_loss: 0.1569 - val_accuracy: 0.9629\n",
      "Epoch 296/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0520 - accuracy: 0.9839 - val_loss: 0.1588 - val_accuracy: 0.9635\n",
      "Epoch 297/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0512 - accuracy: 0.9841 - val_loss: 0.1539 - val_accuracy: 0.9639\n",
      "Epoch 298/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0489 - accuracy: 0.9848 - val_loss: 0.1606 - val_accuracy: 0.9631\n",
      "Epoch 299/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0503 - accuracy: 0.9843 - val_loss: 0.1629 - val_accuracy: 0.9630\n",
      "Epoch 300/300\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0497 - accuracy: 0.9845 - val_loss: 0.1565 - val_accuracy: 0.9639\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=4056, epochs=300, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed59b1",
   "metadata": {},
   "source": [
    "# 4.Evaluate Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28731190",
   "metadata": {},
   "source": [
    "## 4.1. Plotting Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "971e6040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loss Curve\n",
    "plt.plot(history.history['loss'], label = 'Training Loss')\n",
    "plt.plot(history.history['val_loss'], label = \"Test Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0c193ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#accuracy Curve\n",
    "plt.plot(history.history['accuracy'], label = 'Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = \"Test Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806270f7",
   "metadata": {},
   "source": [
    "## 4.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f86c3441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2417/2417 [==============================] - 5s 2ms/step - loss: 0.1565 - accuracy: 0.9639\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741a02aa",
   "metadata": {},
   "source": [
    "##  4.3 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8465627",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68ff1c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.6192649e-13, 9.9989617e-01, 3.6859561e-12, ..., 4.0685706e-17,\n",
       "        4.4175780e-14, 6.0868338e-16],\n",
       "       [9.8522830e-01, 7.4109573e-07, 8.5230880e-09, ..., 1.2917138e-10,\n",
       "        1.2215824e-11, 5.4466615e-11],\n",
       "       [9.9999678e-01, 4.0495008e-13, 6.2584064e-08, ..., 2.6750822e-20,\n",
       "        1.1732168e-10, 2.3758113e-28],\n",
       "       ...,\n",
       "       [9.9818891e-01, 1.3291025e-06, 2.6117499e-09, ..., 6.7359075e-08,\n",
       "        2.3391276e-12, 3.5606362e-10],\n",
       "       [1.3497018e-24, 9.0658697e-23, 1.2807333e-29, ..., 0.0000000e+00,\n",
       "        4.9481687e-38, 6.3548458e-27],\n",
       "       [9.6245849e-01, 4.3541090e-07, 4.4273690e-07, ..., 1.2600985e-09,\n",
       "        2.4723819e-09, 1.7900362e-12]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2db297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5d9df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
